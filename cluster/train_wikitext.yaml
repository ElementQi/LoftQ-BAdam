description: wikitext

target:
  service: aml
  # name: tscience-a100-80g-eastus
  name: A100-80G-PCIE-westus3
  # name: V10032G
  # name: A100EastUS
  # name: openai-A10080G
  # name: A10080G
  # name: gpu-v100-32g
  # name: gpu-a100-80g


environment:
  image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel
  image_setup:
    - apt-get -y update
    - apt-get -y install wget
    - apt-get -y install git
    - apt-get -y install git-lfs
  setup:
    - pip install transformers==4.31
    - pip install accelerate==0.23
    - pip install git+https://github.com/huggingface/peft
    - pip install evaluate scikit-learn scipy typing_extensions einops
    - pip install datasets sentencepiece setuptools rouge-score nltk openai
    - pip install tensorboard tensorboardX
    - pip install bitsandbytes

storage:
  output:
    storage_account_name: tsinterns
    container_name: t-qingzhang
    mount_dir: /mnt/t-qingzhang

code:
  local_dir: ../

jobs:
- name: wikitext_llama2_7b_bit16
  sku: 1xG4
  process_count_per_node: 1
  submit_args:
    container_args:
      cpus: 32
  command:
    - accelerate launch train_clm.py --num_bits 16 --num_iter 0 --learning_rate 2e-4 --reduced_rank 64 --seed 202 --dataset_name wikitext --dataset_config wikitext-2-raw-v1 --path_to_model_zoo /mnt/t-qingru/yixiaoli_model_zoo --output_dir /mnt/t-qingru/exp_results_c_more/ --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --model_name_or_path meta-llama/Llama-2-13b-hf --do_train --do_eval --logging_steps 50 --save_steps 1000000 --evaluation_strategy epoch --report_to tensorboard --overwrite_output_dir --block_size 512
    - accelerate launch train_clm.py --num_bits 16 --num_iter 0 --learning_rate 2e-4 --reduced_rank 64 --seed 303 --dataset_name wikitext --dataset_config wikitext-2-raw-v1 --path_to_model_zoo /mnt/t-qingru/yixiaoli_model_zoo --output_dir /mnt/t-qingru/exp_results_c_more/ --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --model_name_or_path meta-llama/Llama-2-13b-hf --do_train --do_eval --logging_steps 50 --save_steps 1000000 --evaluation_strategy epoch --report_to tensorboard --overwrite_output_dir --block_size 512
    - accelerate launch train_clm.py --num_bits 16 --num_iter 0 --learning_rate 2e-4 --reduced_rank 128 --seed 202 --dataset_name wikitext --dataset_config wikitext-2-raw-v1 --path_to_model_zoo /mnt/t-qingru/yixiaoli_model_zoo --output_dir /mnt/t-qingru/exp_results_c_more/ --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --model_name_or_path meta-llama/Llama-2-13b-hf --do_train --do_eval --logging_steps 50 --save_steps 1000000 --evaluation_strategy epoch --report_to tensorboard --overwrite_output_dir --block_size 512
    - accelerate launch train_clm.py --num_bits 16 --num_iter 0 --learning_rate 2e-4 --reduced_rank 128 --seed 303 --dataset_name wikitext --dataset_config wikitext-2-raw-v1 --path_to_model_zoo /mnt/t-qingru/yixiaoli_model_zoo --output_dir /mnt/t-qingru/exp_results_c_more/ --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --model_name_or_path meta-llama/Llama-2-13b-hf --do_train --do_eval --logging_steps 50 --save_steps 1000000 --evaluation_strategy epoch --report_to tensorboard --overwrite_output_dir --block_size 512

- name: wikitext_llama2_13b_iter0
  sku: 1xG4
  process_count_per_node: 1
  submit_args:
    container_args:
      cpus: 32
  command:
    - accelerate launch train_clm.py --num_bits 4 --num_iter 0 --learning_rate 5e-6 --reduced_rank 64 --seed 202 --dataset_name wikitext --dataset_config wikitext-2-raw-v1 --path_to_model_zoo /mnt/t-qingru/yixiaoli_model_zoo --output_dir /mnt/t-qingru/exp_results_c_more/ --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --model_name_or_path meta-llama/Llama-2-13b-hf --do_train --do_eval --logging_steps 50 --save_steps 1000000 --evaluation_strategy epoch --report_to tensorboard --overwrite_output_dir --block_size 512
    - accelerate launch train_clm.py --num_bits 4 --num_iter 0 --learning_rate 5e-6 --reduced_rank 128 --seed 101 --dataset_name wikitext --dataset_config wikitext-2-raw-v1 --path_to_model_zoo /mnt/t-qingru/yixiaoli_model_zoo --output_dir /mnt/t-qingru/exp_results_c_more/ --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --model_name_or_path meta-llama/Llama-2-13b-hf --do_train --do_eval --logging_steps 50 --save_steps 1000000 --evaluation_strategy epoch --report_to tensorboard --overwrite_output_dir --block_size 512
    - accelerate launch train_clm.py --num_bits 4 --num_iter 0 --learning_rate 5e-6 --reduced_rank 128 --seed 202 --dataset_name wikitext --dataset_config wikitext-2-raw-v1 --path_to_model_zoo /mnt/t-qingru/yixiaoli_model_zoo --output_dir /mnt/t-qingru/exp_results_c_more/ --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --model_name_or_path meta-llama/Llama-2-13b-hf --do_train --do_eval --logging_steps 50 --save_steps 1000000 --evaluation_strategy epoch --report_to tensorboard --overwrite_output_dir --block_size 512

- name: wikitext_llama2_13b_iter1
  sku: 1xG4
  process_count_per_node: 1
  submit_args:
    container_args:
      cpus: 32
  command:
    - accelerate launch train_clm.py --num_bits 4 --num_iter 1 --learning_rate 2e-4 --reduced_rank 128 --seed 101 --dataset_name wikitext --dataset_config wikitext-2-raw-v1 --path_to_model_zoo /mnt/t-qingru/yixiaoli_model_zoo --output_dir /mnt/t-qingru/exp_results_c_more/ --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --model_name_or_path meta-llama/Llama-2-13b-hf --do_train --do_eval --logging_steps 50 --save_steps 1000000 --evaluation_strategy epoch --report_to tensorboard --overwrite_output_dir --block_size 512
    - accelerate launch train_clm.py --num_bits 2 --num_iter 1 --learning_rate 2e-4 --reduced_rank 64 --seed 101 --dataset_name wikitext --dataset_config wikitext-2-raw-v1 --path_to_model_zoo /mnt/t-qingru/yixiaoli_model_zoo --output_dir /mnt/t-qingru/exp_results_c_more/ --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --model_name_or_path meta-llama/Llama-2-13b-hf --do_train --do_eval --logging_steps 50 --save_steps 1000000 --evaluation_strategy epoch --report_to tensorboard --overwrite_output_dir --block_size 512

- name: wikitext_llama2_13b_iter5
  sku: 1xG4
  process_count_per_node: 1
  submit_args:
    container_args:
      cpus: 32
  command:
    - accelerate launch train_clm.py --num_bits 4 --num_iter 5 --learning_rate 2e-4 --reduced_rank 128 --seed 101 --dataset_name wikitext --dataset_config wikitext-2-raw-v1 --path_to_model_zoo /mnt/t-qingru/yixiaoli_model_zoo --output_dir /mnt/t-qingru/exp_results_c_more/ --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --model_name_or_path meta-llama/Llama-2-13b-hf --do_train --do_eval --logging_steps 50 --save_steps 1000000 --evaluation_strategy epoch --report_to tensorboard --overwrite_output_dir --block_size 512
    - accelerate launch train_clm.py --num_bits 2 --num_iter 5 --learning_rate 2e-4 --reduced_rank 64 --seed 101 --dataset_name wikitext --dataset_config wikitext-2-raw-v1 --path_to_model_zoo /mnt/t-qingru/yixiaoli_model_zoo --output_dir /mnt/t-qingru/exp_results_c_more/ --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --model_name_or_path meta-llama/Llama-2-13b-hf --do_train --do_eval --logging_steps 50 --save_steps 1000000 --evaluation_strategy epoch --report_to tensorboard --overwrite_output_dir --block_size 512

